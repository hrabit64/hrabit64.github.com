---
title: 유니티 ML_Agents 공부해보기 - 기본구조(1) Agent
categories: ML_Agents
tags: [ML_Agents,Unity]
toc: true
toc_label: 목차
toc_sticky: true
classes: wide
---







## 0. 개요

이 블로그의 모든 글은 "텐서플로와 유니티 ML-Agents로 배우는 강화학습" 도서를 공부하며 정리한 내용들이다. 대부분의 내용들과 코드들은 이 책을 정리한 것이다.  또한 이런 좋은 책을 내주신 저자분들께 감사드린다.

도서 링크: https://wikibook.co.kr/tensorflow-mlagents/



 기본적으로 강화학습 알고리즘을 이용해 개발을 하기 위해서는 알고리즘을 구현하는 것도 중요하지만, 알고리즘을 학습할 수 있는 환경을 구성하는 것도 중요하다. 하지만 실질적으로 알고리즘을 구현하고, 학습하는 환경까지 구현하기에는 너무 많은 힘이 든다.

 이러한 문제점을 해결하기 위해 2017년 9월 ML-Agents가 등장하였다. 이 ML-Agents를 사용하면 기존보다 더 쉽게 강화학습을 구현할 수 있으며, 또한 기본적으로 내장 알고리즘을 지원하기 때문에 쉽게 강화학습에 입문해볼 수 도 있다.

 이 ML-Agents에서 내장 알고리즘을 사용하는 방법은 다음과 같다.

1. 빌드된 유니티 환경 준비하기
2. 빌드된 환경의 에이전트를 기본적으로 제공하는 알고리즘을 이용해 학습
3. 학습이 종료되면 학습된 딥러닝 모델(확장자명 .nn) 생성
4. 생성된 모델을 에이전트에 적용하여 사용

기본 내장 알고리즘을 사용하지 않고 파이썬으로 구성된 에이전트를 활용할 수도 있다.

ML-Agents을 사용하면 빌드된 환경에서 파이썬으로 **상태,보상**에 대한 정보를 제공하면, 파이썬은 그 에 대한 **액션**에 대한 정보를 주고 받으며 학습이 이루어진다.

![유니티 mlagents구조](/image/다운로드.png)
=======


![다운로드](\assets\image\다운로드.png)

출처 : https://blogs.unity3d.com/kr/2017/09/19/introducing-unity-machine-learning-agents/



위 이미지는 ML-Agents의 구성을 나타낸 것이다. 이 그림을 살펴보면 ML-Agents는 다음과 같이 구성되어 있다.

**●에이전트(Agent)**

**●브레인(brain)**

**●아카데미(Academy)**

오늘 이 글에서는 에이전트에 대해서 공부할 것 이다.



## 1.에이전트(Agent)의 기본 구조

에이전트는 환경 내에서 행동을 취하는 요소이다. 환경과 관측을 담은 상태를 기반으로 의사결정을 하는 요소이다.

유니티 블로그에서는 다음과 같이 설명하고 있다.



"*각각의 에이전트는 고유의 상태 및 관측 값을 가지고 있고, 환경 내에서 고유의 행동을 하며 환경 내부에서 일어나는 이벤트에 따라 고유의 보상을 받습니다. 각 에이전트의 행동은 해당 에이전트가 연결되어 있는 브레인에 의해 결정됩니다.*"



 이 ML-Agents에서는 기본적으로 다음과 같이 4가지 함수를 가지고 있다.

```c#
public class TemplateAgent : Agent{
    public override void CollectObservations()
    {

    }
    public overrid void AgentAction(float[] vectorAction, string textAction)
    {

    }
    public overrid void AgentReset()
    {

    }
    public overrid void AgentOnDone()
    {

    }
}
```



## 2. CollectObservations()

**CollectObservations** 함수는 에이전트의 수치적 관측에 값을 추가하는 함수이다. **AddVectorObs()** 함수를 사용하여 수치적 관측에 값을 추가할 수 있다.

```c#
public override void CollectObservations()
{
    AddVectorObs(test.position.x);
    AddVectorObs(test.position.z);       
}
```



## 3. AgentAction(float[] vectorAction, string textAction)

**AgentAction()** 함수는 에이전트의 행동을 입력받고(float[] vectorAction), 어떻게 명령을 수행할지 결정(string textAction)하는 함수이다.

강화학습의 기본 개념을 공부했다면 에이전트의 **Action**에는 **이산적인 행동**과 **연속적인 행동** 으로 2가지 방식이 존재한다는 것을 알고 있을 것 이다.



● **이산적인 행동**

행동의 선택지가 주어지며, 그중 하나를 선택하는 방식

● **연속적인 행동**

선택지마다 특정 값을 수치로 입력하여 행동하는 방식



따라서 이산적인 행동인지 연속적인 행동인지에 따라 작성법이 다르다.



### 3-1 이산적인 행동(Discrete Action)

유니티의 ML-Agents에는 **브랜치(Branch)**라는 개념이 존재한다. 브랜치는 행동의 종류를 나누는 기준이며 이산적인 행동을 나타내야하기 때문에 **정수값**으로 구성된다.

예를 들어 에이전트가 x,y,z축으로 움직여야 한다면 각각의 축을 Branch 0, Branch 1,Branch 2 로 설정할 수 있을 것이다. 또한 각각의 축에 가하는 힘을 0,1,2으로 표현한다면 다음과 같이 코드를 구현할 수 있다.

```c#
public overrid void AgentAction(float[] vectorAction, string textAction)
{
    //입력받은 vectorAction은 모든 브랜치에 해당하는 액션의 값들이 float형으로 저장되어 있음.
    //브랜치는 정수로 구성되기에 Mathf.FloorToInt 함수를 통해 정수로 변환해줌
    int movement_X = Mathf.FloorToInt(vectorAction[0]);
    int movement_Y = Mathf.FloorToInt(vectorAction[1]);
    int movement_Z = Mathf.FloorToInt(vectorAction[2]);

    //x축
    if (movement_X == 0){
        directionX = -1;
    }
    if (movement_X == 1){
     	directionX = 2;
    }    
    if (movement_X == 2){
        directionX = 1;
    }
    //y축
    if (movement_Y == 0){
        directionY = -1;
    }
    if (movement_Y == 1){
     	directionY = 2;
    }    
    if (movement_Y == 2){
        directionY = 1;
    }
    //z축
    if (movement_Z == 0){
        directionZ = -1;
    }
    if (movement_Z == 1){
     	directionZ = 2;
    }    
    if (movement_Z == 2){
        directionZ = 1;
    }

    //위 수식에서 x,y,z축에 정해진 힘을 주는 함수 Vector3는 x,y,z축 방향에 대한 벡터의 크기를 나타냄
    gameObject.GetComponent<Rigidbody>().AddForce(new Vector3(directionX, directionY,directionZ))

}
```



### 3-2 연속적인 행동(Continuous Action)

이산적인 행동에서 브랜치를 사용했지만, 연속적인 행동에서는 브랜치의 개념이 적용되지 않는다.  대신 이름 처럼 **연속적인 값**을 가질 수 있다. 만약 -1~1까지의 값을 가진다고 하면 -1~1까지의 모든 실수값이 될 수 있는것이다.

따라서 위 코드를 연속적인 행동을 하는 코드로 수정한다면 다음과 같을 것이다.

```c#
public overrid void AgentAction(float[] vectorAction, string textAction)
{
	//Mathf.Clamp(Value,min,max) 입력 받은 값이 최소/최대 크기를 넘지 않도록 해주는 함수이다.
    float force_X = Mathf.Clamp(vectorAction[0],-1.0f,1.0f);
    float force_y = Mathf.Clamp(vectorAction[1],-1.0f,1.0f);
    float force_z = Mathf.Clamp(vectorAction[2],-1.0f,1.0f);

    gameObject.GetComponent<Rigidbody>().AddForce(new Vector3(force_X,force_y,force_z))

}
```



### 3-3 AddReward()

강화학습에서 에이전트가 행동을 취했다면 그에 맞는 보상을 주어야 할 것이다. ML-Agents에서는 이를 **AddReward()** 함수를 사용하여 보상을 줄 수 있다.

```c#
//만약 x축의 위치가 1이 된다면 1.0 만큼의 보상을 줄 것이다.
if (this.transform.position.x == 1.0f)
{
    AddReward(1.0f);
    //Done() 함수를 사용하여 게임 한판을 끝내준다.
    Done();
}
//만약 x축의 위치가 -1이 된다면 -1.0 만큼의 보상을 줄 것이다.
if (this.transform.position.x == -1.0f)
{
    AddReward(-1.0f);
    Done();
}
```

강화학습에서 에이전트는 보상을 최대화 하도록 학습된다. 따라서 위 코드와 같이 구현한다면 x축의 위치가 1.0이 되도록 학습될 것이다.



## 4. AgentReset()

**AgentReset()** 함수는 **AddRweard()** 함수를 설명할 때 나왔던 **Done()**을 실행하여 게임 한판이 종료되었을 때 호출된다. 에이전트의 상태를 어떻게 초기화할지 설정하는 부분이다.

만약 게임 한판이 종료 되었을 때, 위치를 (0,0,0)으로 이동시키고 싶다면 다음과 같이 구현하면 된다.

```c#
public overrid void AgentReset()
{
      transform.position = new Vector3(0f,0f,0f);  
}
```



## 5. AgentOnDone()

**AgentOnDone()** 함수는 에이전트를 더이상 사용하지 않을 때, 나중에 후술할 Brain에 알리는 함수이다.

```c#
public overrid void AgentOnDone()
{
	Destroy(gameObject);c
}
```



## 6. 정리

오늘 쓴 내용을 내 나름대로 정리하면 다음과 같다.



ML-Agents에서 에이전트는 **주인공** 역할이다.

**CollectObservations()** 함수를 이용해 에이전트에게 수치적 관측을 전달하여 에이전트가 판단할 근거를 제공하고,

**AgentAction(float[] vectorAction, string textAction)** 함수를 이용해 에이전트에게 어떻게 행동할지 **Action**을 지정해준다.

이때 **AddReward()** 변수로 보상을 주고, 종료 조건에 도달하면 **Done()** 함수를 이용해 게임을 종료한다.

이때, **Done()** 함수로 인해 게임이 종료 됬다면, **AgentReset()** 변수에 따라 에이전트를 초기화한다.

만약 에이전트가 필요 없어진다면 **AgentOnDone()** 을 통해 **brain**에 알리고 삭제한다.